use crate::token::Token;
use crate::token::TokenType;
use regex::Regex;

enum State {
    Initial,
    Name,
    Integer,
    Float
}

pub struct Parser {}
impl Parser {
    pub fn parse(
        buffer : &String,
        token_type : TokenType)
    -> Option<Token>
    {
        let _type = match token_type {
            TokenType::Infer => match &buffer[..] {
                ")" => Some(TokenType::RBracket),
                "(" => Some(TokenType::LBracket),
                "}" => Some(TokenType::RBrace),
                "{" => Some(TokenType::LBrace),
                "]" => Some(TokenType::RCol),
                "[" => Some(TokenType::LCol),
                "+" => Some(TokenType::Plus),
                "-" => Some(TokenType::Minus),
                "*" => Some(TokenType::Mult),
                "/" => Some(TokenType::Div),
                ";" => Some(TokenType::PComma),
                "," => Some(TokenType::Comma),
                "==" => Some(TokenType::EQ),
                "!=" => Some(TokenType::NEQ),
                ">=" => Some(TokenType::GE),
                "<=" => Some(TokenType::LE),
                "int" => Some(TokenType::Int),
                "float" => Some(TokenType::Float),
                "char" => Some(TokenType::Char),
                _ => {
                    let re = Regex::new(r"[a-zA-Z0-9_]+").unwrap();
                    match re.is_match(buffer) {
                        true => Some(TokenType::Id),
                        false => None
                    }
                }
            },
            _ => Some(token_type)
        };

        match _type {
            None => None,
            Some(T) => Some(Token {
                lexeme : buffer.clone(),
                _type : T,
                line : -1
            })
        }
    }
}
    
pub struct Lexer<'a> {
    source : String,
    it : std::str::Chars<'a>,
    begin : usize,
    end : usize,
    nline : i32,
    symbol : char,
    tokens : Vec<Token>,
    state : i32
}

impl Lexer<'_> {
    pub fn go_next(&mut self)
    {
        self.begin += 1;
        self.symbol = self.it.next().expect("Iterator already reached end");
    }

    pub fn push(
        &mut self,
        token_type : TokenType,
        include_current : bool)
    -> bool
    {
        let _end = if include_current { self.end+1 } else { self.end };
        let lexeme = &self.source[self.begin.._end];
        match Token::new(lexeme, token_type, self.nline) {
            Some(t) => {
                self.tokens.push(t);
                self.end = _end;
                true
            },
            None => false
        }
    }

    pub fn parse(filename : &str)
    -> Vec<Token>
    {
        let binding = std::fs::read_to_string(filename)
            .unwrap_or_else(|err| {
                eprintln!("Parsing error : {err}");
                std::process::exit(1);
            });

        let source = binding.clone();
        let mut it = binding.as_str().chars();
        let symbol = it.next().unwrap();

        let mut lexer = Lexer {
            source : source,
            it : it,
            begin : 0,
            end : 0,
            nline : 1,
            symbol : symbol,
            tokens : Vec::new(),
            state : 0,
        };

        while lexer.end <= lexer.source.len() {
            let mut should_go_next = true;
            if lexer.symbol == '\n' {
                lexer.nline += 1;
            }
            
            match lexer.state {
                0 => {
                    if String::from("()[]{}+*/,;!").find(lexer.symbol).is_some() {
                        lexer.push(TokenType::Infer, true);
                    }
                    else if String::from("-=><").find(lexer.symbol).is_some() {
                        lexer.state = 1;    
                    }
                }

                1 => {
                    if !lexer.push(TokenType::Infer, true) {
                        lexer.push(TokenType::Infer, false);
                        should_go_next = false;
                    }
                    lexer.state = 0;
                }

                _ => {}
            }

            if should_go_next {
                lexer.go_next();
            }
        }
        
        lexer.tokens
    }
}
